{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from platform import python_version\n",
    "\n",
    "# Definitions of certain constants\n",
    "DAY = datetime.timedelta(days=1)\n",
    "\n",
    "# The following are \"irredeemable\"\n",
    "RAIN_DESC = \"Rain perturbing etcp\"\n",
    "SIMUL_DESC = \"Software simulation\"\n",
    "IRR_DESC = \"Irrigation perturbing etcp\"\n",
    "NULL_PROFILE_DESC = \"Null profile value\"\n",
    "DATA_BLIP_DESC = \"Profile data blip\"\n",
    "LARGE_PROFILE_DIP_DESC = \"Large profile dip\"\n",
    "ETCP_POS_DESC = \"Etcp is positive\"\n",
    "ETCP_OUTLIERS_DESC = \"Etcp outliers\"\n",
    "LUX_DESC = \"Luxurious water uptake\"\n",
    "BAD_KCP_DESC = \"Unacceptable kcp\"\n",
    "UNREDEEMABLE = [RAIN_DESC, SIMUL_DESC, IRR_DESC, NULL_PROFILE_DESC, DATA_BLIP_DESC,\n",
    "                LARGE_PROFILE_DIP_DESC, ETCP_POS_DESC, ETCP_OUTLIERS_DESC, LUX_DESC, BAD_KCP_DESC]\n",
    "\n",
    "# The following  are \"redeemable\"\n",
    "HU_STUCK_DESC = \"Heat Units `stuck`\"\n",
    "ETO_STUCK_DESC = \"Eto `stuck`\"\n",
    "ETC_STUCK_DESC = \"Stuck etc due to stuck eto\"\n",
    "REDEEMABLE = [HU_STUCK_DESC, ETO_STUCK_DESC, ETC_STUCK_DESC]\n",
    "\n",
    "ETO_MAX = 12\n",
    "KCP_MAX = 0.8\n",
    "ETCP_MAX = ETO_MAX * KCP_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henri is using Python version 3.6.8.  You are using Python version 3.6.8.\n",
      "Henri is using numpy version 1.15.4.  You are using numpy version 1.16.2.\n",
      "Henri is using pandas version 0.24.0.  You are using pandas version 0.23.4.\n",
      "Henri is using matplotlib version 2.2.3.  You are using matplotlib version 2.2.3.\n"
     ]
    }
   ],
   "source": [
    "print(\"Henri is using Python version 3.6.8.  You are using Python version {}.\".format(python_version()))\n",
    "print(\"Henri is using numpy version 1.15.4.  You are using numpy version {}.\".format(np.__version__))\n",
    "print(\"Henri is using pandas version 0.24.0.  You are using pandas version {}.\".format(pd.__version__))\n",
    "print(\"Henri is using matplotlib version 2.2.3.  You are using matplotlib version {}.\".format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a helper function called `\"flagger\"`\n",
    "\n",
    "This `\"flagger\"` function will set the flag value equal to 1 for faulty data.  The dates for which the flag value is equal to 1 will not be used in the new calculation of $k_{cp} = \\frac{\\mathrm{ET}_{cp}}{\\mathrm{ET}_o}$.\n",
    "\n",
    "`\"flagger\"` will also add a description in the `\"description\"` column about why a particular date has been flagged.\n",
    "\n",
    "The `\"flagger\"` function only operates on the `\"df_flag\"` DataFrame (instantiated later in this notebook) which only has two columns:\n",
    "1. `\"binary_value\"`\n",
    "2. `\"description\"`  \n",
    "`\"df_flag\"` also has a DateTime Index.\n",
    "\n",
    "Later on, if desired, we can then merge the `\"df_flag\"` DataFrame with the main DataFrame containing all our data.  Our merging will take place in such a fashion that we merge entries corresponding to identical dates. (In short, we merge on the Index of our DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagger(bad_dates, brief_desc, bin_value=0):\n",
    "    \"\"\"\n",
    "    Flag bad_dates with a binary value of 1 and append a brief description about why bad_dates were flagged.\n",
    "    \n",
    "    Parameters:\n",
    "    bad_dates (pandas.core.indexes.datetimes.DatetimeIndex):  Dates for which we cannot calculate k_cp because our readings were perturbed and rendered unuseful.\n",
    "    brief_desc (str):  A very short description about why bad_dates were flagged.\n",
    "    bin_value (int):  The binary value.  If Eto is imputed, Etc and heat_units are stuck, we can still get away with a new calculation of kcp; thus set binary_value=0 for such redeemable events.\n",
    "    \n",
    "    Returns:\n",
    "    None.  It updates the DataFrame storing all the information related to flagging.  In this case the DataFrame is called `df_flag`    \n",
    "    \"\"\"\n",
    "    if df_flag.loc[bad_dates, \"description\"].str.contains(brief_desc).all(axis=0):\n",
    "        # The bad_dates have already been flagged for the reason given in brief_desc.\n",
    "        # No use in duplicating brief_desc contents in the description column.\n",
    "        # Therefore redundant information in the df_flag DataFrame is avoided.\n",
    "        print(\"You have already flagged these dates for the reason given in `brief_desc`; No flagging has taken place.\")\n",
    "        return\n",
    "    else:\n",
    "        for d in bad_dates:\n",
    "            cond = (brief_desc in df_flag.loc[d, \"description\"])\n",
    "            if (df_flag.loc[d, \"binary_value\"] == 0) & (bin_value == 0) & (cond is True):\n",
    "                continue\n",
    "            elif (df_flag.loc[d, \"binary_value\"] == 0) & (bin_value == 0) & (cond is False):\n",
    "                df_flag.loc[d, \"description\"] += (\" \" + brief_desc + \".\")\n",
    "            elif (df_flag.loc[d, \"binary_value\"] == 0) & (bin_value == 1) & (cond is True):\n",
    "                df_flag.loc[d, \"binary_value\"] = 1\n",
    "            elif (df_flag.loc[d, \"binary_value\"] == 0) & (bin_value == 1) & (cond is False):\n",
    "                df_flag.loc[d, \"binary_value\"] = 1\n",
    "                df_flag.loc[d, \"description\"] += (\" \" + brief_desc + \".\")\n",
    "            elif (df_flag.loc[d, \"binary_value\"] == 1) & (bin_value == 0) & (cond is True):\n",
    "                continue\n",
    "            elif (df_flag.loc[d, \"binary_value\"] == 1) & (bin_value == 0) & (cond is False):\n",
    "                df_flag.loc[d, \"description\"] += (\" \" + brief_desc + \".\")\n",
    "            elif (df_flag.loc[d, \"binary_value\"] == 1) & (bin_value == 1) & (cond is True):\n",
    "                continue\n",
    "            else:  # (df_flag.loc[d, \"binary_value\"] == 1) & (bin_value == 1) & (cond is False)\n",
    "                df_flag.loc[d, \"description\"] += (\" \" + brief_desc + \".\")\n",
    "        df_flag.loc[bad_dates, \"description\"] = df_flag.loc[:, \"description\"].apply(lambda s: s.lstrip().rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a helper function named `\"reporter\"`:\n",
    "\n",
    "This function prints statements regarding:\n",
    "1. How much data was lost due to a specific flagging operation.\n",
    "2. (Optional, `Default=False`) How much data remains useful after all the flagging that has taken place in the entire notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporter(brief_desc, remaining=False):\n",
    "    tally = df_flag[\"description\"].str.contains(brief_desc).sum()\n",
    "    n_tot_entries = len(df_flag.index)\n",
    "    perc = tally / n_tot_entries * 100\n",
    "    print(\"{:.1f}% of data is affected due to [{}].\".format(perc, brief_desc))\n",
    "    \n",
    "    if remaining:\n",
    "        calc = 100 - df_flag[\"binary_value\"].sum()/len(df_flag.index)*100\n",
    "        print(\"After all the flagging that has taken place in this entire notebook, only {:.0f}% of your data is useful.\".format(calc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The _Kouebokkeveld_ longterm data, and imputer definition:\n",
    "\n",
    "In the following, we:\n",
    "1. Define a DataFrame storing the _Kouebokkeveld_ long-term data.\n",
    "2. Define the `kbv_imputer` function that tries to impute the long-term data into stuck/repeating $\\mathrm{ET}_o$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_week = np.array([25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
    "                          35, 36, 37, 38, 39, 40, 41, 42, 43, 44, \n",
    "                          45, 46, 47, 48, 49,  1,  2,  3,  4,  5,\n",
    "                           6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
    "                          16, 17, 18, 19, 20, 21, 22, 23, 24])\n",
    "\n",
    "kbv_eto = np.array([2.30, 2.30, 2.30, 2.30, 2.30, 2.40, 2.50, 2.65, 2.80, 3.10,\n",
    "                    3.40, 3.65, 4.00, 4.40, 4.80, 5.30, 5.80, 6.30, 6.70, 7.10,\n",
    "                    7.60, 8.00, 8.30, 8.60, 8.80, 8.90, 8.90, 8.90, 8.80, 8.70,\n",
    "                    8.50, 8.30, 8.00, 7.50, 7.00, 6.50, 5.80, 5.20, 4.70, 4.30,\n",
    "                    3.70, 3.40, 3.10, 2.80, 2.50, 2.45, 2.40, 2.35, 2.30])\n",
    "\n",
    "df_kbv = pd.DataFrame(data=kbv_eto, index=calendar_week, columns=[\"kbv_eto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbv_imputer(flagged_dates, dataframe, column_to_be_imputed, flag_dataframe):\n",
    "    IMPUTED_ETO = \"Imputed eto\"\n",
    "    for d in flagged_dates:\n",
    "        week_number = d.isocalendar()[1]\n",
    "        try:\n",
    "            dataframe.loc[d, [column_to_be_imputed]] = df_kbv.loc[week_number, \"kbv_eto\"]\n",
    "            for description in UNREDEEMABLE:\n",
    "                if description in flag_dataframe.loc[d, \"description\"]:\n",
    "                    break\n",
    "            else:\n",
    "                flag_dataframe.loc[d, \"binary_value\"] = 0  # we have 'salvaged' an entry.\n",
    "                flag_dataframe.loc[d, \"description\"] = flag_dataframe.loc[d, \"description\"].replace(ETO_STUCK_DESC, IMPUTED_ETO)\n",
    "        except KeyError:\n",
    "            dataframe.loc[d, column_to_be_imputed] = np.nan\n",
    "    return dataframe, flag_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Crop Coefficients & $k_{cp}$ flagging function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_month = np.array([7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6])\n",
    "penman_kcp = np.array([0.10, 0.30, 0.40, 0.60, 0.73, 0.88, 0.95, 0.95, 0.95, 0.70, 0.40, 0.20])\n",
    "row_4 =      np.array([0.61, 0.73, 0.88, 0.95, 0.95, 0.95, 0.95, 0.90, 0.80, 0.40, 0.30, 0.10])\n",
    "\n",
    "accepted_kcp_norm = pd.DataFrame(index=calendar_month, data=row_4, columns=[\"norm_kcp\"])\n",
    "accepted_kcp_norm.index.name = \"calendar_month\"\n",
    "\n",
    "def calculate_kcp_deviation(dataframe):\n",
    "    dataframe[\"kcp_perc_deviation\"] = 0.0\n",
    "    for d in dataframe.index:\n",
    "        month_from_datetime = d.month\n",
    "        associated_kcp_norm = accepted_kcp_norm.loc[month_from_datetime, \"norm_kcp\"]\n",
    "        empirical_kcp = dataframe.loc[d, \"kcp\"]\n",
    "        perc_deviation = np.abs((empirical_kcp - associated_kcp_norm)/associated_kcp_norm) * 100.0\n",
    "        dataframe.loc[d, \"kcp_perc_deviation\"] = perc_deviation\n",
    "    return dataframe[\"kcp_perc_deviation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Probe-ID you are interested in:\n",
    "\n",
    "Valid options are:\n",
    "* `\"P-370\"`\n",
    "* `\"P-371\"`\n",
    "* `\"P-372\"`\n",
    "* `\"P-384\"`\n",
    "* `\"P-391\"`\n",
    "* `\"P-392\"`\n",
    "* `\"P-891\"`\n",
    "\n",
    "You can change the Probe-ID in the following code-cell, and please remember to specify it in string format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_id = \"P-392\"\n",
    "assert isinstance(probe_id, str), \"variable probe_id must be of type string!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data from our Excel file and store it in a `pandas DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice in the following code cell, we extract the daily data for the probe of interest.\n",
    "\n",
    "- In the for-loop, we remove the unnecessary leading white-space at the beginning of each column name.  After the redundant white-space has been removed, we assign the redundant-free column names to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Golden_Delicious_daily_data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-26e74294f3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Golden_Delicious_daily_data.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'0'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"o\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, **kwds)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     return io.parse(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Golden_Delicious_daily_data.xlsx'"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(\"Golden_Delicious_daily_data_new.xlsx\", sheet_name=probe_id, index_col=0, parse_dates=True)\n",
    "new_columns = []\n",
    "for c in data.columns:\n",
    "    if '0' in c:\n",
    "        c = c.replace(\"0\", \"o\")\n",
    "    new_columns.append(c.lstrip())\n",
    "data.columns = new_columns\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data columns are not of interest to us for our analysis:\n",
    "* `rzone`\n",
    "* `available`\n",
    "* `days_left`\n",
    "* `deficit_current`\n",
    "* `rzm`\n",
    "* `fcap`\n",
    "* `deficit_want`\n",
    "* `refill`\n",
    "* `eto_forecast_yr`\n",
    "\n",
    "Therefore, we are going to drop these columns from the `data` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=[\"rzone\", \"available\", \"days_left\", \"deficit_current\", \"rzm\", \n",
    "                  \"fcap\", \"deficit_want\", \"refill\", \"eto_forecast_yr\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"calendar_week\"] = data.index.to_series().apply(lambda d: d.isocalendar()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create standalone `df_flag` dataframe\n",
    "\n",
    "In the following code we create a standalone dataframe which will store flag values as well as descriptions on why a particular date was flagged as being inappropriate for our new calculation of $k_{cp}$.  At first, we initialise all our dates to a flag value of 0.  If during our analysis we realise that there are dates with junk data, we will update the flag value to be 1, and also provide a brief description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flag = pd.DataFrame(index=data.index, columns=[\"binary_value\", \"description\"])\n",
    "df_flag[\"binary_value\"] = 0\n",
    "df_flag[\"description\"] = str()  # we initialise this column with an empty string `\"\"`\n",
    "df_flag.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `[\"etc\", \"eto\"]` versus time\n",
    "\n",
    "Notice that `\"etc\"` is column G in the Excel spreadsheet.  \n",
    "`\"eto\"` is column I in the Excel spreadsheet.  \n",
    "`\"etcp\"` lives in column J of the Excel spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(9, 3.5)\n",
    "\n",
    "ax.plot(data.index, data[\"etc\"], color=\"blue\", label=\"$\\mathrm{ET}_c$\")\n",
    "ax.plot(data.index, data[\"eto\"], color=\"green\", label=\"$\\mathrm{ET}_o$\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), linestyle=\"--\", color=\"red\", alpha=0.6, label=\"2018-08-01\")\n",
    "ax.set_ylabel(\"$\\mathrm{ET}_c$ and $\\mathrm{ET}_o$\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(\"Evapotranspiration versus Time\")\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"eto_diff1\"] = data[\"eto\"].diff(periods=1)\n",
    "data[\"eto_diff2\"] = data[\"eto\"].diff(periods=2)\n",
    "condition = (data[\"eto_diff1\"] == 0.0) | (data[\"eto_diff2\"] == 0)  # bitwise OR operation\n",
    "bad_eto_days = data[condition].index\n",
    "\n",
    "data.loc[bad_eto_days, [\"eto\"]] = np.nan\n",
    "\n",
    "flagger(bad_dates=bad_eto_days, brief_desc=ETO_STUCK_DESC, bin_value=1)\n",
    "reporter(brief_desc=ETO_STUCK_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute _Kouebokkeveld_ data into stuck `eto` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, df_flag = kbv_imputer(flagged_dates=bad_eto_days, dataframe=data,\n",
    "                    column_to_be_imputed=\"eto\", flag_dataframe=df_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in df_flag.index:\n",
    "    print(\"{} <---> {}\".format(df_flag.loc[d, \"binary_value\"], df_flag.loc[d, \"description\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag `etc` values that are stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[bad_eto_days, [\"etc\"]] = np.nan\n",
    "\n",
    "flagger(bad_dates=bad_eto_days, brief_desc=ETC_STUCK_DESC, bin_value=0)\n",
    "reporter(brief_desc=ETC_STUCK_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are time intervals for which `\"et0\"` (and also `\"heat_units\"`) are stuck: they repeat identical values for a long duration of time.  Obviously, it is not possible to get such repetitive values when considering the fact that weather data is very random.  The data entries associated with these repeating values are flagged.\n",
    "\n",
    "It is rather unfortunate that a relatively large volume of data is lost when flagging these \"stuck\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(9, 3.5)\n",
    "\n",
    "ax.plot(data.index, data[\"etc\"], color=\"blue\", label=\"Remaining $\\mathrm{ET}_c$\")\n",
    "ax.plot(data.index, data[\"eto\"], color=\"green\", label=\"Remaining $\\mathrm{ET}_o$\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), linestyle=\"--\", color=\"red\", alpha=0.6, label=\"2018-08-01\")\n",
    "ax.set_ylabel(\"$\\mathrm{ET}_c$ and $\\mathrm{ET}_o$\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_title(\"$\\mathrm{ET}_c$ and $\\mathrm{ET}_o$ after flagging and imputation.\")\n",
    "ax.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eto_max = max(data[\"eto\"])\n",
    "print(\"The maximum (valid) eto is equal to: {:.1f}.\".format(eto_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `\"rain\"` versus time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we flag rain events in which:\n",
    "* `\"rain\"` > 2 mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (data[\"rain\"] > 2)\n",
    "flagged_rain_dates = data[condition].index\n",
    "\n",
    "flagger(bad_dates=flagged_rain_dates, brief_desc=RAIN_DESC, bin_value=1)\n",
    "reporter(brief_desc=RAIN_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "ax.bar(x=data.index, height=data[\"rain\"], color=\"magenta\", label=\"rain\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"rain [mm]\")\n",
    "ax.set_title(\"rain versus time\")\n",
    "ax.scatter(flagged_rain_dates, data.loc[flagged_rain_dates, [\"rain\"]], label=\"Rain > 2 mm (flagged)\", \n",
    "           c=\"black\", marker=6, s=10, alpha=1)\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"blue\", ls=\"--\", alpha=0.4, label=\"2018-08-01\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The great majority of rain events are flagged.\n",
    "\n",
    "There appears to be few rain events in which: rain < 2 mm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `\"Total Irrigation\"`\n",
    "\n",
    "We need to flag data entries corresponding to irrigation events because it distorts our `\"profile\"` and `\"etcp\"` waterbalance readings.  This is somewhat complicated by the possibility of a **farmer logging an irrigation event on the wrong date.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flag an irrigation data entry:\n",
    "1. If the farmer logged irrigation taking place for that day, **AND**\n",
    "2. If the cerresponding $\\mathrm{ET}_{cp} > 0.5\\cdot\\mathrm{ET}_c$, **AND**\n",
    "3. If there is no rain for that particular day: rain == 0.\n",
    "\n",
    "Let us implement this in the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (data[\"total_irrig\"] > 0) & (data[\"etcp\"] > 0.5*data[\"etc\"]) & (data[\"rain\"] == 0)\n",
    "flag_irrigation_dates = data[conditions].index\n",
    "\n",
    "flagger(bad_dates=flag_irrigation_dates, brief_desc=IRR_DESC, bin_value=1)\n",
    "reporter(brief_desc=IRR_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "ax.bar(data.index, data[\"total_irrig\"], color=\"magenta\", label=\"Irrigation\")\n",
    "ax.scatter(flag_irrigation_dates, data.loc[flag_irrigation_dates, [\"total_irrig\"]], label=\"Flagged Irr events\", \n",
    "           c=\"black\", marker=\"o\", s=5, alpha=1)\n",
    "ax.scatter(flagged_rain_dates, data.loc[flagged_rain_dates, [\"total_irrig\"]], label=\"rain > 2 mm\",\n",
    "           c=\"orange\", marker=\"^\", s=10, alpha=1)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Total Irrigation [mm]\")\n",
    "ax.set_title(\"Total Irrigation versus Time\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"blue\", ls=\"--\", alpha=0.4, label=\"2018-08-01\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure looks wrong.  There appears to be many irrigation events that are not flagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate `\"rzm_source\"`:  \"`software`\" versus \"`Electronic Probe`\".\n",
    "\n",
    "Basically we want to flag the entries for which the column `\"rzm_source\"` contains the description `software`.  We do not want to build our model from simulated data, but rather from actual probe readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = data[\"rzm_source\"].str.contains(\"software\")\n",
    "flag_software_dates = data[condition].index\n",
    "\n",
    "flagger(bad_dates=flag_software_dates, brief_desc=SIMUL_DESC, bin_value=1)\n",
    "reporter(brief_desc=\"Software simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `\"profile\"` versus time\n",
    "\n",
    "Notice that `\"profile\"` corresponds to column R in the Excel spreadsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `\"profile\"` column, there are certain entries containing 0.0; these entries correspond to missing data.  For these missing `\"profile\"` entries, we replace the 0.0's with `NaN`'s (Not a Number).  We also flag these missing `\"profile\"` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"profile\"].replace(0.0, np.nan, inplace=True)  # replace missing entries with NaN\n",
    "\n",
    "condition = data[\"profile\"].isnull()\n",
    "bad_profile_days = data[condition].index\n",
    "\n",
    "flagger(bad_dates=bad_profile_days, brief_desc=NULL_PROFILE_DESC, bin_value=1)\n",
    "reporter(brief_desc=NULL_PROFILE_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"profile_difference\"] = data[\"profile\"].diff()\n",
    "\n",
    "data_blip_days = []\n",
    "for d in data.index:\n",
    "    try:\n",
    "        if (data.loc[d, \"profile_difference\"] < 0) and pd.isnull(data.loc[d + DAY, \"profile\"]):\n",
    "            data_blip_days.append(d)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "data_blip_days = pd.to_datetime(data_blip_days)\n",
    "flagger(bad_dates=data_blip_days, brief_desc=DATA_BLIP_DESC, bin_value=1)\n",
    "reporter(brief_desc=DATA_BLIP_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Profile')\n",
    "ax.set_title(\"Profile versus Time.\")\n",
    "ax.plot(data.index, data[\"profile\"], color=\"blue\", label=\"Profile\", lw=\"1\")\n",
    "ax.scatter(x=data_blip_days, y=data.loc[data_blip_days, [\"profile\"]], s=50, color=\"black\", \n",
    "            marker=\"*\", label=\"Data blips\", edgecolors=\"red\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"black\", alpha=0.4, linestyle=\"--\", label=\"2018-08-01\")\n",
    "\n",
    "for d in flag_irrigation_dates:\n",
    "    ax.axvline(x=d, color=\"pink\", alpha=1.0, linestyle=\"-\", linewidth=0.3)\n",
    "ax.axvline(x=flag_irrigation_dates[0], color=\"pink\", alpha=1.0, linestyle=\"-\", linewidth=0.3, label=\"Flagged Irr\")\n",
    "\n",
    "for d in flagged_rain_dates:\n",
    "    ax.axvline(x=d, color=\"lime\", alpha=1.0, linestyle=\"-\", linewidth=0.3)\n",
    "ax.axvline(x=flagged_rain_dates[0], color=\"lime\", alpha=1.0, linestyle=\"-\", linewidth=0.3, label=\"rain > 2 mm\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for the dates adjacent to missing data gaps, there is always a strange slanting in the `\"profile\"` value.  These slanting dips are indicated by the red stars in the above plot.  The data entries associated with these slanting dips are also flagged.  These are data blips and imply that the apple tree had a massive water uptake via absorption through its roots, but physiologically this is not possible.\n",
    "\n",
    "There are also other profile readings that appear suspicious because of the following pattern: profile(t-1) is some value, say $k$; profile(t) dips very low from profile(t-1); profile(t+1) is close to the value of profile(t-1).  Up next, we try to flag these \"large dips\" that appear suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data_blip_days ,[\"profile_difference\"]] = np.nan\n",
    "\n",
    "negative_differences = data[data[\"profile_difference\"] < 0][\"profile_difference\"].values\n",
    "percentile_value = np.quantile(negative_differences, q=[0.01, 0.02, 0.03, 0.04, 0.05,\n",
    "                                                        0.06, 0.07, 0.08, 0.09, 0.10])[4]\n",
    "\n",
    "large_dip_days = []\n",
    "for d in data.index:\n",
    "    try:\n",
    "        if (data.loc[d, \"profile_difference\"] < percentile_value) and (data.loc[d + DAY, \"profile_difference\"] > 0):\n",
    "            large_dip_days.append(d)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "large_dip_days = pd.to_datetime(large_dip_days)\n",
    "flagger(bad_dates=large_dip_days, brief_desc=LARGE_PROFILE_DIP_DESC, bin_value=1)\n",
    "reporter(brief_desc=LARGE_PROFILE_DIP_DESC)\n",
    "print(len(large_dip_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Profile')\n",
    "ax.set_title(\"Profile versus Time.\")\n",
    "ax.plot(data.index, data[\"profile\"], color=\"blue\", label=\"Profile\", lw=1)\n",
    "ax.scatter(x=data_blip_days, y=data.loc[data_blip_days, [\"profile\"]], s=50, color=\"black\", \n",
    "            marker=\"*\", label=\"Data blips\", edgecolors=\"red\")\n",
    "ax.scatter(x=large_dip_days, y=data.loc[large_dip_days, [\"profile\"]], s=50, color=\"black\", \n",
    "            marker=\"X\", label=\"'Large' Dips\", edgecolors=\"green\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"black\", alpha=0.5, linestyle=\"--\", label=\"2018-08-01\")\n",
    "\n",
    "for d in flag_irrigation_dates:\n",
    "    ax.axvline(x=d, color=\"pink\", alpha=1.0, linestyle=\"-\", linewidth=0.3)\n",
    "ax.axvline(x=flag_irrigation_dates[0], color=\"pink\", alpha=1.0, linestyle=\"-\", linewidth=0.3, label=\"Flagged Irr\")\n",
    "\n",
    "for d in flagged_rain_dates:\n",
    "    ax.axvline(x=d, color=\"lime\", alpha=1.0, linestyle=\"-\", linewidth=0.3)\n",
    "ax.axvline(x=flagged_rain_dates[0], color=\"lime\", alpha=1.0, linestyle=\"-\", linewidth=0.3, label=\"rain > 2 mm\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `\"heat_units\"` versus Time\n",
    "\n",
    "Notice that heat_units corresponds to column B of the excile spreadsheet.\n",
    "\n",
    "There are many dates for which the value of `\"heat_units\"` is jammed and repeats for a long time interval.  This is due to faulty weatherstation data.  Entries for which `\"heat_units\"` values repeat are flagged, and the actual `\"heat_units\"` values are replaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"hu_diff1\"] = data[\"heat_units\"].diff(periods=1)\n",
    "# data[\"hu_diff2\"] = data[\"heat_units\"].diff(periods=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = (data[\"hu_diff1\"] == 0.0) | (data[\"hu_diff2\"] == 0)  # bitwise OR operation\n",
    "condition = data[\"heat_units\"] == 0\n",
    "bad_hu_days = data[condition].index\n",
    "\n",
    "flagger(bad_dates=bad_hu_days, brief_desc=HU_STUCK_DESC, bin_value=0)\n",
    "reporter(brief_desc=HU_STUCK_DESC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[bad_hu_days, [\"heat_units\"]] = np.nan\n",
    "\n",
    "data.loc[:, \"heat_units\"].interpolate(method=\"nearest\", axis=0, inplace=True)\n",
    "print(data.loc[bad_hu_days, [\"heat_units\"]])\n",
    "\n",
    "condition = data[\"heat_units\"] == 0.0\n",
    "print(\"The number of zero values is: {}.\".format(condition.sum()))\n",
    "\n",
    "data.loc[:, \"heat_units\"].fillna(value=0.0, inplace=True)\n",
    "print(data.loc[bad_hu_days, [\"heat_units\"]])\n",
    "condition = data[\"heat_units\"] == 0.0\n",
    "print(\"The number of zero values is: {}.\".format(condition.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[bad_hu_days, \"heat_units\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative(s):\n",
    "    new = s.to_frame()\n",
    "    new[\"cumulative\"] = 0.0\n",
    "    for stamp in new.index:\n",
    "        if (stamp.month == 8) and (stamp.day == 1):\n",
    "            new.loc[stamp, [\"cumulative\"]] = 0.0\n",
    "        elif (stamp.month == 8) and (stamp.day == 2):\n",
    "            new.loc[stamp, [\"cumulative\"]] = 0.0 + new.loc[stamp][\"heat_units\"]\n",
    "        else:\n",
    "            new.loc[stamp, [\"cumulative\"]] = new.loc[(stamp - DAY)][\"cumulative\"] + new.loc[stamp][\"heat_units\"]\n",
    "    return new[\"cumulative\"]\n",
    "\n",
    "\n",
    "data[\"cumul_heat_units\"] = cumulative(data[\"heat_units\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 3)\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "color = \"blue\"\n",
    "ax1.set_title(\"GDD versus Time\")\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Heat Units (GDD)', color=color)\n",
    "pl1 = ax1.bar(data.index, data[\"heat_units\"], color=color, label=\"Heat Units\")\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.axvline(x=datetime.datetime(2018, 8 , 1), color=\"red\", ls=\"--\", alpha=0.6, label=\"2018-08-01\")\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axis that shares the same x-axis\n",
    "color=\"green\"\n",
    "ax2.set_ylabel(\"Cumulative GDD\", color=color)\n",
    "pl2 = ax2.plot(data.index, data[\"cumul_heat_units\"], color=color, label=\"Cumulative GDD\", lw=1)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "# added these three lines\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc=0)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the above plot, there are many gaps due to faulty weather station data.  Therefore the green curve representing cumulative growing-degree-days is also affected and not accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = data[\"heat_units\"] == 0.0\n",
    "\n",
    "n_equals_zero = condition.sum()\n",
    "n_samples = len(data.index)\n",
    "perc_equals_zero = n_equals_zero * 100.0 / n_samples\n",
    "print(\"Percentage of 0's is: {:.1f}%.\".format(perc_equals_zero))\n",
    "print(\"Total sample size is: {}.\".format(n_samples))\n",
    "print(\"Total number of null samples is: {}.\".format(n_equals_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `\"etcp\"` versus time\n",
    "\n",
    "Notice that `\"etcp\"` belongs to column J in the Excel file.  \n",
    "`\"etcp\"` is defined as the difference between consecutive `\"profile\"` readings:\n",
    "\n",
    "$$\n",
    "\\mathrm{ET}_{cp}(t) = \\mathrm{Profile}(t) - \\mathrm{Profile}(t-1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are mostly interested in the $\\mathrm{ET}_{cp}$ entries for which $\\mathrm{ET}_{cp} < 0$.  These negative entries reflect incidents in which water was lost from the soil due to: (1) Water Drainage, (2) Luxurious water uptake, (3) Normal water uptake, (4) and Drought-stress.\n",
    "\n",
    "We are not interested in dates in which $\\mathrm{ET}_{cp}$ is perturbed by irrigation and/or rain.  For such dates where $\\mathrm{ET}_{cp}$ is perturbed, we expect $\\mathrm{ET}_{cp} \\ge 0$.  Therefore, data entries corresponding to $\\mathrm{ET}_{cp} \\ge 0$ are flagged.  Furthermore, all $\\mathrm{ET}_{cp} \\ge 0$ entries are set to `NaN` values (Not a Number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = data[\"etcp\"] >= 0.0\n",
    "bad_etcp_days = data[condition].index\n",
    "\n",
    "flagger(bad_dates=bad_etcp_days, brief_desc=ETCP_POS_DESC, bin_value=1)\n",
    "reporter(brief_desc=ETCP_POS_DESC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set the $\\mathrm{ET}_{cp}$ values for which _all_ the dates in the `df_flag` DataFrame is flagged, due to whatever reason, to `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df_flag[\"binary_value\"] == 1\n",
    "flagged_dates = df_flag[condition].index\n",
    "\n",
    "data.loc[flagged_dates, \"etcp\"] = np.nan\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we are now only left with the $\\mathrm{ET}_{cp} < 0$ entries (i.e. entries associated with water drainage, luxurious/normal water uptake, and drought-stress).  For simplicity, we multiply these remaining $\\mathrm{ET}_{cp}$ values with -1 so that henceforth we only work with positive values of $\\mathrm{ET}_{cp}$ (which is a little bit more convenient for programming purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"etcp\"] = data[\"etcp\"].multiply(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8.6, 3)\n",
    "\n",
    "ax.scatter(data.index, data[\"etcp\"], color=\"red\", label=\"Remaining $\\mathrm{ET}_{cp}$\", \n",
    "        marker=\"o\", s=10, edgecolors=\"black\", linewidth=1, alpha=0.6)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"$\\mathrm{ET}_{cp}$\")\n",
    "ax.set_title(\"$\\mathrm{ET}_{cp}$ versus time\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"blue\", alpha=0.6, ls=\"--\", label=\"2018-08-01\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove $\\mathrm{ET}_{cp}$ outliers\n",
    "\n",
    "As can be seen in the figure above, there are some outliers still present in the remaining $\\mathrm{ET}_{cp}$ dataset.\n",
    "\n",
    "These outliers are most likely associated with phases of water-drainage and luxurious water uptake.  We, on the other hand, are only interested in phases corresponding to normal water uptake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data[\"etcp\"].quantile([0.50, 0.60, 0.70, 0.75, 0.80, 0.85, 0.90, 0.91, 0.92, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an educated guess, let us accept a maximum $\\mathrm{ET}_o$ of 12.0 mm.  Let us make another educated guess and allow for a maximum $k_{cp}$ value of 0.8 (of course, these educated guesses vary from cultivar to cultivar).  This implies that the maximum allowed value for $\\mathrm{ET}_{cp}$ is as follows:\n",
    "\n",
    "$$\n",
    "\\mathrm{max}(\\mathrm{ET}_{cp}) = 0.8 \\times 12 = 9.6\\,\\mathrm{mm}\n",
    "$$\n",
    "\n",
    "Consequently, we flag all $\\mathrm{ET}_{cp} > 9.6\\, \\mathrm{mm}$ data entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform a filtering process to get rid of all $\\mathrm{ET}_{cp}$ values higher than $9.6\\,\\mathrm{mm}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = data[\"etcp\"] >= ETCP_MAX\n",
    "bad_high_etcp_dates = data[condition].index\n",
    "\n",
    "data.loc[bad_high_etcp_dates, [\"etcp\"]] = np.nan\n",
    "\n",
    "flagger(bad_dates=bad_high_etcp_dates, brief_desc=ETCP_OUTLIERS_DESC, bin_value=1)\n",
    "reporter(brief_desc=ETCP_OUTLIERS_DESC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove dates for which luxurious water uptake is present\n",
    "\n",
    "But remember, we only tolerate $\\mathrm{ET}_{cp}$ values for which $k_{cp} \\le 0.8$.  Therefore, to ensure that all luxurious water-uptake phases are completely flagged, we perform another flagging operation that flags data entries for which $\\mathrm{ET}_{cp} > 0.8\\cdot\\mathrm{ET}_o$ (for extra insurance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = data[\"etcp\"] > data[\"eto\"].mul(KCP_MAX, fill_value=np.nan)\n",
    "luxurious_dates = data[condition].index\n",
    "\n",
    "data.loc[luxurious_dates, [\"etcp\"]] = np.nan\n",
    "\n",
    "flagger(bad_dates=luxurious_dates, brief_desc=LUX_DESC, bin_value=1)\n",
    "reporter(brief_desc=LUX_DESC, remaining=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the remaining $\\mathrm{ET}_{cp}$ entries which are _hopefully_ valid data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (df_flag[\"binary_value\"] == 0)\n",
    "useful_dates = data[condition].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "ax.scatter(useful_dates, data.loc[useful_dates, [\"eto\"]], color=\"green\", label=\"$\\mathrm{ET}_o$\", \n",
    "        marker=\"o\", s=10, edgecolors=\"black\", linewidth=1, alpha=0.6)\n",
    "ax.scatter(useful_dates, data.loc[useful_dates, [\"etcp\"]], color=\"red\", label=\"$\\mathrm{ET}_{cp}$\", \n",
    "        marker=\"s\", s=10, edgecolors=\"black\", linewidth=1, alpha=0.6)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"$\\mathrm{ET}_o$ and $\\mathrm{ET}_{cp}$\")\n",
    "ax.set_title(\"$\\mathrm{ET}_o$ and $\\mathrm{ET}_{cp}$ versus Time\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"blue\", alpha=0.6, ls=\"--\", label=\"2018-08-01\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How must we flag events of drought-stress?\n",
    "\n",
    "One line of reasoning is that during phases of drough-stress we expect very small changes in the waterbalance \"Profile\" readings.  According to this logic, we expect for drought-stress that:\n",
    "\n",
    "$$\n",
    "\\mathrm{Profile}(t) - \\mathrm{Profile}(t-1) = \\mathrm{ET}_{cp}(t) < \\varepsilon\n",
    "$$\n",
    "\n",
    "where $\\varepsilon$ is a relatively small value, such as, for example, $\\varepsilon = 0.1$.\n",
    "\n",
    "It was suggested we evaluate $k_{cp}$ and compare it to the accepted norm (on file).  If $k_{cp}$ differs by more than 50% +- then flagging should take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough calculation of $k_{cp}$\n",
    "\n",
    "Basically, after all the necessary flagging has been carried out, we can proceed to calculate $k_{cp}$ as follows:\n",
    "\n",
    "$$\n",
    "k_{cp} = \\frac{\\mathrm{ET}_{cp}}{\\mathrm{ET}_o}\n",
    "$$\n",
    "\n",
    "In the plot below, we show the calculated $k_{cp}$ for entries containing both valid $\\mathrm{ET}_{cp}$ and $\\mathrm{ET}_o$ entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[\"kcp\"] = data[\"etcp\"].div(data[\"eto\"], fill_value=np.nan)\n",
    "print(data[\"kcp\"].notna().sum())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "ax.scatter(data.index, data[\"kcp\"], color=\"purple\", label=\"$k_{cp}$\",\n",
    "        marker=\"D\", s=10, edgecolors=\"black\", linewidth=1, alpha=0.6)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"$k_{cp}$\")\n",
    "ax.set_title(\"$k_{cp}$ versus time\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"blue\", alpha=0.6, ls=\"--\", label=\"2018-08-01\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag $k_{cp}$ values that deviate by more than +/- 50% from the accepted norm:\n",
    "\n",
    "This is achieved with the help of the `kcp_flagger` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to check for NaN in a column, use:  df.isnull(), or, s.isnull()\n",
    "perc_series = calculate_kcp_deviation(data)\n",
    "condition = (perc_series.isnull()) | (perc_series > 50)\n",
    "bad_calc_kcp_dates = data[condition].index\n",
    "\n",
    "data.loc[bad_calc_kcp_dates, \"kcp\"] = np.nan\n",
    "flagger(bad_dates=bad_calc_kcp_dates, brief_desc=BAD_KCP_DESC, bin_value=1)\n",
    "reporter(brief_desc=BAD_KCP_DESC, remaining=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replot kcp versus time.  Evaluate after bad kcp values have been flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "ax.scatter(data.index, data[\"kcp\"], color=\"gold\", label=\"Remaining $k_{cp}$\",\n",
    "        marker=\"D\", s=10, edgecolors=\"black\", linewidth=1, alpha=0.6)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"$k_{cp}$\")\n",
    "ax.set_title(\"$k_{cp}$ versus time\")\n",
    "ax.axvline(x=datetime.datetime(2018, 8, 1), color=\"blue\", alpha=0.6, ls=\"--\", label=\"2018-08-01\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the original `data` DataFrame with the `df_flag` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = pd.concat([data, df_flag], axis=1, join=\"inner\")\n",
    "master_data.head(n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only retain month and day; discard year.  Replot $k_{cp}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = master_data[\"binary_value\"] == 0\n",
    "useful_dates = master_data[condition].index\n",
    "starting_year = useful_dates[0].year\n",
    "\n",
    "new_dates = []\n",
    "for d in useful_dates:\n",
    "    extracted_month = d.month\n",
    "    if 8 <= extracted_month <= 12:\n",
    "        new_dates.append(datetime.datetime(year=starting_year, month=d.month, day=d.day))\n",
    "    else:\n",
    "        new_dates.append(datetime.datetime(year=starting_year+1, month=d.month, day=d.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 3)\n",
    "plt.locator_params(axis=\"x\", n_bins=12)\n",
    "\n",
    "mdates.AutoDateFormatter\n",
    "mdates.AutoDateLocator\n",
    "ax.scatter(new_dates, master_data.loc[useful_dates, \"kcp\"], color=\"gold\",\n",
    "           label=\"Remaining $k_{cp}$\", marker=\"D\", s=10, edgecolors=\"black\",\n",
    "           linewidth=1, alpha=0.6)\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"$k_{cp}$\")\n",
    "ax.set_title(\"$k_{cp}$ versus time\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%d'))\n",
    "ax.set_xlim(left=datetime.datetime(year=starting_year, month=8, day=1),\n",
    "            right=datetime.datetime(year=starting_year+1, month=7, day=31))\n",
    "ax.legend()\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
